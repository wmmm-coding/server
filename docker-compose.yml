services:
  # PostgreSQL con pgvector
  postgres:
    image: pgvector/pgvector:pg16
    container_name: wmm_postgres
    restart: unless-stopped
    environment:
      DB_POSTGRESDB_HOST: ${DB_POSTGRESDB_HOST}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      WMM_USER: ${WMM_USER}
      WMM_PASSWORD: ${WMM_PASSWORD}
      WMM_DB: ${WMM_DB}
      WMM_VECTOR_DB: ${WMM_VECTOR_DB}

      POSTGRES_PORT: ${POSTGRES_PORT:-5432}
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - "5432:5432"
    volumes:
      # Usa il volume esistente con hash
      - pg_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    networks:
      - wmm-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 5

  # n8n Workflow Automation
  n8n:
    build:
      context: ./n8n
    container_name: wmm_n8n
    restart: unless-stopped
    user: "1000:1000"
    ports:
      - "5678:5678"  # Esporre n8n quando in network sharing mode
    environment:
      - N8N_HOST=${N8N_HOST:-localhost}
      - N8N_PORT=${N8N_PORT:-5678}
      - N8N_PROTOCOL=${N8N_PROTOCOL:-http}
      - N8N_EDITOR_BASE_URL=${N8N_EDITOR_BASE_URL:-http://localhost:5678/}
      - N8N_RUNNERS_ENABLED=true
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
      - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
      - WEBHOOK_URL=${WEBHOOK_URL:-http://localhost:5679/}
      #  Timezone
      - GENERIC_TIMEZONE=Europe/Rome
      - TZ=Europe/Rome
      # n8n Configuration:
      - N8N_METRICS=true    # Enables metrics endpoint for monitoring n8n performance
      - N8N_LOG_LEVEL=debug  # Sets logging to debug level for more detailed logs
      - N8N_LOG_OUTPUT=console # Directs logs to container's console output (viewable with docker logs)
      - N8N_SECURE_COOKIE=${N8N_SECURE_COOKIE:-false}
      - DB_TYPE=sqlite
      - DB_SQLITE_DATABASE=/home/node/.n8n/database.sqlite

      # üîß PROXY CONFIGURAZIONE SPECIFICA PER OLLAMA
      # - ALL_PROXY=socks5://172.19.0.10:1055
      # - SOCKS_PROXY=socks5://172.19.0.10:1055
      # - HTTP_PROXY=http://172.19.0.10:1055
      # - HTTPS_PROXY=http://172.19.0.10:1055
      # - NO_PROXY=localhost,127.0.0.1,postgres,pgadmin,search,172.19.0.10,172.20.0.0/16,100.111.217.81,100.0.0.0/8,100.111.217.81:11434
      
      # üîß FORZATURA BYPASS OLLAMA
      - OLLAMA_HOST=100.111.217.81:11434
      - OLLAMA_NO_PROXY=true

      # üî• TIMEOUT CONFIGURATION (KEEP ONLY ESSENTIAL)
      - N8N_DEFAULT_HTTP_REQUEST_TIMEOUT=600000     # 10 minuti
      - N8N_EXECUTION_TIMEOUT=1800                  # 30 minuti workflow
      
      # üî• NODE.JS OPTIMIZATION
      - NODE_OPTIONS=--max-old-space-size=4096
      
      # # üîß FORZATURA DIRETTA PER OLLAMA
      # - NODE_NO_PROXY=100.111.217.81,100.0.0.0/8
      
      # # üîß OLLAMA CLIENT SETTINGS
      # - OLLAMA_BASE_URL=http://100.111.217.81:11434



    volumes:
      - n8n_data_volume:/home/node/.n8n:rw
    # === TAILSCALE DIRECT ACCESS ===
    # n8n accede direttamente agli IP Tailscale tramite routing della rete tailscale-net
    networks:
      wmm-network:
      tailscale-net:
        ipv4_address: 172.19.0.20  # IP fisso per routing
    depends_on:
      postgres:
        condition: service_healthy
      # tailscale-proxy:
      #   condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:5678/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Caddy Reverse Proxy
  caddy:
    image: caddy:2-alpine
    container_name: wmm_caddy-proxy
    restart: unless-stopped
    environment:
      - CADDY_LOG_LEVEL=debug
    profiles:
      - production
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./config/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy-config:/config
      - caddy_logs:/var/log/caddy  
    networks:
      - wmm-network
    depends_on:
      n8n:
        condition: service_healthy
      pgadmin:
        condition: service_started
    healthcheck:
      test: ["CMD", "caddy", "list-certificates"]
      interval: 30s
      timeout: 10s
      retries: 3

  pgadmin:
    image: dpage/pgadmin4:9.5
    container_name: pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_PORT: 80
      PGADMIN_DEFAULT_EMAIL: admin@web-marketing-manager.it
      PGADMIN_DEFAULT_PASSWORD: admin_password
      PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION: "False"  # ‚úÖ AGGIUNTO
      PGADMIN_CONFIG_WTF_CSRF_ENABLED: "False"
      PGADMIN_CONFIG_WTF_CSRF_CHECK_DEFAULT: "False"
      PGADMIN_CONFIG_SESSION_COOKIE_SECURE: "False"
      PGADMIN_CONFIG_MFA_ENABLED: "False"
      PGADMIN_CONFIG_SERVER_MODE: "True"
      # Only set these variables if they are defined
     
    ports:
      - "5050:80"
    volumes:
      - pg_admin_data:/var/lib/pgadmin
    networks:
      - wmm-network
    depends_on:
      postgres:
        condition: service_healthy
  
  # tailscale-proxy:
  #   image: tailscale/tailscale:latest
  #   container_name: wmm_tailscale_proxy
  #   hostname: tailscale-proxy-${TAILSCALE_HOSTNAME}
  #   environment:
  #     - TS_AUTHKEY=${TAILSCALE_AUTH_KEY}
  #     - TS_STATE_DIR=/var/lib/tailscale
  #     - TS_USERSPACE=true
  #     - TS_ACCEPT_DNS=true 
  #     - TS_EXTRA_ARGS=--accept-routes --advertise-tags=tag:container --advertise-routes=172.19.0.0/16,172.20.0.0/16
  #   volumes:
  #     - ${PWD}/tailscale-proxy/state:/var/lib/tailscale
  #   networks:
  #     tailscale-net:
  #       ipv4_address: 172.19.0.10  # IP fisso per routing
  #     wmm-network:
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "tailscale", "status"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   command: >
  #     sh -c "
  #       echo 'Starting Tailscale in userspace mode...' &&
  #       tailscaled --tun=userspace-networking --socks5-server=0.0.0.0:1055 --outbound-http-proxy-listen=0.0.0.0:1055 --verbose=1 &
  #       echo 'Waiting for tailscaled to start...' &&
  #       sleep 5 &&
  #       echo 'Authenticating with Tailscale...' &&
  #       until tailscale up $$TS_EXTRA_ARGS; do 
  #         echo 'Retrying authentication...'
  #         sleep 5
  #       done &&
  #       echo 'Tailscale userspace proxy ready!' &&
  #       tailscale status &&
  #       echo 'SOCKS5 proxy listening on port 1055' &&
  #       echo 'HTTP proxy listening on port 1055' &&
  #       echo 'Testing Ollama connection...' &&
  #       wget -q -O - --timeout=5 http://100.111.217.81:11434/api/tags > /dev/null && echo 'Ollama connection OK' || echo 'Ollama connection FAILED' &&
  #       echo 'Port forwarding setup complete' &&
  #       while true; do sleep 3600; done
  #     "
    
  # # ts-commander:
  # #   image: tailscale/tailscale:latest
  # #   container_name: ts-commander
  # #   hostname: ${TAILSCALE_HOSTNAME}
  # #   environment:
  # #     - TS_AUTHKEY=${TAILSCALE_AUTH_KEY}
  # #     - TS_STATE_DIR=/var/lib/tailscale
  # #     - TS_USERSPACE=false
  # #     - TS_EXTRA_ARGS=--advertise-tags=tag:container --advertise-routes=172.20.0.0/16 --accept-routes --reset
  # #   volumes:
  # #     - ${PWD}/ts-commander/state:/var/lib/tailscale
  # #   devices:
  # #     - /dev/net/tun:/dev/net/tun
  # #   ports:
  # #     - "5678:5678"
  # #   cap_add:
  # #     - net_admin
  # #     - sys_module
  # #   networks:
  # #     - wmm-network
  # #     - tailscale-net 
  # #   restart: unless-stopped

  #   # sysctls:
  #   #   - net.ipv4.ip_forward=1

  # #   image: nginx
  #   # network_mode: service:ts-authkey-test

  searxng:
    build:
      context: ./searxng
    container_name: search
    restart: unless-stopped
    ports:
      - "8888:8080"
    volumes:
      - searxng-config:/etc/searxng
      - searxng-data:/var/cache/searxng
      # Bind mount semplice per compatibilit√† universale
      - ./searxng/limiter.toml:/etc/searxng/limiter.toml:ro
      - ./searxng/settings.yml:/etc/searxng/settings.yml:ro
    environment:
      - SEARXNG_BASE_URL=http://localhost:8888/
      # - SEARXNG_SECRET=WMM_SEARCH_SECRET_KEY_CHANGE_THIS
    networks:
      - wmm-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3
        

# Reti Docker
networks:
  wmm-network:
    driver: bridge
    name: wmm-network
    ipam:
      config:
        - subnet: 172.20.0.0/16

  tailscale-net:
    driver: bridge
    name: tailscale-net
    ipam:
      config:
        - subnet: 172.19.0.0/16
# Volumi persistenti - riferimenti ai volumi esistenti
volumes:
  # Contiene i dati di n8n ossia come loggarsi
  n8n_data_volume:
    name: n8n_data_volume
  # Volume PostgreSQL esistente (con hash)
  pg_data:
    name: pg_data

  pg_admin_data:
    name: pg_admin_data

  searxng-config:
    name: searxng_config     

  searxng-data:
    name: searxng_data  
  
  # Volumi Caddy esistenti
  caddy_data:
    name: caddy_data
  
  caddy-config:
    name: caddy_config

  caddy_logs:
    name: caddy_logs
 
  




